"""
ä¸»æ§åˆ¶Agent
è´Ÿè´£ç†è§£ç”¨æˆ·æ„å›¾ï¼Œåè°ƒå…¶ä»–ä¸“ä¸šAgentå·¥ä½œ
"""
import os
import logging
from typing import Dict, List, Any, Optional, Generator, Callable
import json
from qwen_agent.agents import Router
import pandas as pd

# å¯¼å…¥å„ä¸ªä¸“ä¸šAgent
from app.agents.knowledge_agent import KnowledgeAgent
from app.agents.data_agent import DataAgent
from app.agents.sql_agent import SQLAgent
from app.agents.visualization_agent import VisualizationAgent

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class MainAgent:
    """ä¸»æ§åˆ¶Agentï¼Œç†è§£ç”¨æˆ·æ„å›¾å¹¶åè°ƒå…¶ä»–Agentå·¥ä½œ"""
    
    def __init__(self):
        """åˆå§‹åŒ–ä¸»æ§åˆ¶Agent"""
        # è·å–APIå¯†é’¥å’Œæ¨¡å‹åç§°
        api_key = os.getenv("QWEN_API_KEY")
        model_name = os.getenv("QWEN_MODEL", "qwen-max")
        
        # åŸºç¡€LLMé…ç½®
        self.llm_cfg = {
            'model': model_name,
            'model_server': 'dashscope',
            'api_key': api_key,
            'generate_cfg': {
                'max_input_tokens': 12000,  # å¢åŠ æœ€å¤§è¾“å…¥tokenæ•°
                'max_output_tokens': 4000   # å¢åŠ æœ€å¤§è¾“å‡ºtokenæ•°
            }
        }
        
        # åˆå§‹åŒ–å­Agent
        self.knowledge_agent = KnowledgeAgent()
        self.data_agent = DataAgent()
        self.visualization_agent = VisualizationAgent()
        # åˆå§‹åŒ–SQL Agentï¼Œä½†ä¸è¿æ¥æ•°æ®åº“
        self.sql_agent = SQLAgent()  
        
        # è·å–å„ä¸ªAgentçš„Assistantå®ä¾‹ï¼Œç”¨äºRouter
        knowledge_assistant = self.knowledge_agent.knowledge_assistant
        data_assistant = self.data_agent.data_assistant
        visualization_assistant = self.visualization_agent.visualization_assistant
        sql_assistant = self.sql_agent.sql_assistant
        
        # åˆ›å»ºRouter Agentï¼Œé›†æˆæ‰€æœ‰ä¸“ä¸šåŠ©æ‰‹
        self.control_agent = Router(
            llm=self.llm_cfg,
            agents=[knowledge_assistant, data_assistant, visualization_assistant, sql_assistant],
            name='ç¾å¦†é”€å”®åŠ©æ‰‹',
            description='ä¸€ä¸ªä¸“ä¸šçš„ç¾å¦†é”€å”®æ•°æ®åˆ†æå¯¹è¯åŠ©æ‰‹ï¼Œèƒ½å¤Ÿç†è§£æ‚¨çš„éœ€æ±‚å¹¶æä¾›å¤šæ–¹é¢çš„ä¸“ä¸šåˆ†æ'
        )
        
        # ä¼šè¯çŠ¶æ€
        self.session_state = {
            "current_data_path": None,
            "current_database": None,
            "conversation_history": [],
            "last_query_type": None,
            "last_analysis_result": None
        }
        
        # å¯è§†åŒ–é…ç½®
        self.visualization_config = {
            "default_chart_type": "bar",
            "color_theme": "default",
            "show_data_labels": True
        }
    
        # æ·»åŠ ç”¨äºä¿å­˜å¯è§†åŒ–ç»“æœçš„å®ä¾‹å˜é‡
        self._current_visualization_result = None
        self._current_shared_context = None
    
    def _sync_data_between_agents(self):
        """åŒæ­¥å„ä¸ªAgentä¹‹é—´çš„æ•°æ®"""
        if self.data_agent.current_data is not None:
            self.visualization_agent.current_data = self.data_agent.current_data.copy()
            logger.info("å·²å°†æ•°æ®åŒæ­¥åˆ°å¯è§†åŒ–Agent")
        
        # åŒæ­¥æ•°æ®è·¯å¾„ä¿¡æ¯
        if self.session_state.get("current_data_path"):
            self.visualization_agent.data_source = self.session_state["current_data_path"]
    
    def connect_database(self, db_params: Dict[str, Any]) -> bool:
        """è¿æ¥åˆ°æ•°æ®åº“
        
        å‚æ•°:
            db_params: æ•°æ®åº“è¿æ¥å‚æ•°
            
        è¿”å›:
            æ˜¯å¦æˆåŠŸè¿æ¥
        """
        try:
            # è¿æ¥æ•°æ®åº“
            connection_success = self.sql_agent.connect_db(db_params)
            
            if connection_success:
                self.session_state["current_database"] = db_params
                logger.info(f"æˆåŠŸè¿æ¥åˆ°æ•°æ®åº“")
            else:
                logger.error(f"è¿æ¥æ•°æ®åº“å¤±è´¥")
                
            return connection_success
                
        except Exception as e:
            logger.error(f"è¿æ¥æ•°æ®åº“æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return False
    
    def process_query(self, query: str):
        """å¤„ç†ç”¨æˆ·æŸ¥è¯¢ï¼Œæ”¯æŒå¤šä¸“å®¶åä½œå¤„ç†å¤æ‚é—®é¢˜ï¼Œå®æ—¶æµå¼è¾“å‡ºè¿‡ç¨‹ä¸ç»“æœ"""
        # è®°å½•å½“å‰æŸ¥è¯¢åˆ°ä¼šè¯å†å²
        self.session_state["conversation_history"].append({"role": "user", "content": query})
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦å±•ç¤ºå…¨éƒ¨Agentèƒ½åŠ›
        should_use_all_agents = self._should_use_all_agents(query)
        
        if should_use_all_agents:
            # ä½¿ç”¨å›ºå®šçš„4ä¸ªAgentåä½œæµç¨‹
            yield from self._process_with_all_agents(query)
        else:
            # ä½¿ç”¨åŸæœ‰çš„Routerè§„åˆ’æµç¨‹
            yield from self._process_with_router(query)
    
    def _should_use_all_agents(self, query: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ä½¿ç”¨æ‰€æœ‰4ä¸ªAgentè¿›è¡Œåä½œåˆ†æ"""
        # æ£€æŸ¥æŸ¥è¯¢ä¸­æ˜¯å¦åŒ…å«éœ€è¦å…¨é¢åˆ†æçš„å…³é”®è¯
        comprehensive_keywords = [
            "å…¨é¢åˆ†æ", "å®Œæ•´åˆ†æ", "æ·±å…¥åˆ†æ", "ç»¼åˆåˆ†æ",
            "ä»å¤šä¸ªè§’åº¦", "å¤šç»´åº¦", "å…¨æ–¹ä½", 
            "è¡Œä¸šèƒŒæ™¯", "æ•°æ®æŸ¥è¯¢", "ç»Ÿè®¡åˆ†æ", "å¯è§†åŒ–",
            "å®Œæ•´æŠ¥å‘Š", "è¯¦ç»†æŠ¥å‘Š", "ä¸“ä¸šåˆ†ææŠ¥å‘Š",
            "å±•ç¤ºæ‰€æœ‰èƒ½åŠ›", "å…¨éƒ¨åŠŸèƒ½", "å®Œæ•´æµç¨‹"
        ]
        
        query_lower = query.lower()
        
        # å¦‚æœæŸ¥è¯¢åŒ…å«è¿™äº›å…³é”®è¯ï¼Œä½¿ç”¨å…¨éƒ¨Agent
        for keyword in comprehensive_keywords:
            if keyword in query_lower:
                return True
        
        # å¦‚æœæŸ¥è¯¢æ¯”è¾ƒå¤æ‚ï¼ˆå­—æ•°è¾ƒå¤šï¼‰ï¼Œä¹Ÿå€¾å‘äºä½¿ç”¨å…¨éƒ¨Agent
        if len(query) > 20:
            return True
            
        return False
    
    def _process_with_all_agents(self, query: str):
        """ä½¿ç”¨æ‰€æœ‰4ä¸ªAgentæŒ‰å›ºå®šé¡ºåºè¿›è¡Œåä½œåˆ†æ"""
        # å®šä¹‰å›ºå®šçš„Agentè°ƒåº¦é¡ºåº
        expert_sequence = [
            {"type": "knowledge", "name": "ç¾å¦†è¡Œä¸šçŸ¥è¯†ä¸“å®¶"},
            {"type": "sql", "name": "SQLä¸“å®¶"},
            {"type": "data", "name": "æ•°æ®åˆ†æä¸“å®¶"},
            {"type": "visualization", "name": "æ•°æ®å¯è§†åŒ–ä¸“å®¶"}
        ]
        
        # å‘é€è®¡åˆ’ä¿¡æ¯
        plan_content = f"""æ‰§è¡Œè®¡åˆ’: [ç¾å¦†è¡Œä¸šçŸ¥è¯†ä¸“å®¶] -> [SQLä¸“å®¶] -> [æ•°æ®åˆ†æä¸“å®¶] -> [æ•°æ®å¯è§†åŒ–ä¸“å®¶]

è¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„ç¾å¦†é”€å”®æ•°æ®åˆ†ææµç¨‹ï¼š
1. ç¾å¦†è¡Œä¸šçŸ¥è¯†ä¸“å®¶: æä¾›ç›¸å…³è¡Œä¸šèƒŒæ™¯çŸ¥è¯†å’Œä¸“ä¸šè§è§£
2. SQLä¸“å®¶: æ ¹æ®éœ€æ±‚ç”Ÿæˆç›¸åº”çš„æ•°æ®æŸ¥è¯¢è¯­å¥
3. æ•°æ®åˆ†æä¸“å®¶: å¯¹è·å–çš„æ•°æ®è¿›è¡Œæ·±å…¥ç»Ÿè®¡åˆ†æ
4. æ•°æ®å¯è§†åŒ–ä¸“å®¶: å°†åˆ†æç»“æœè½¬åŒ–ä¸ºç›´è§‚çš„å›¾è¡¨å±•ç¤º

è¿™ä¸ªæµç¨‹å°†å…¨é¢å±•ç¤ºæˆ‘ä»¬ç³»ç»Ÿçš„å®Œæ•´èƒ½åŠ›ã€‚"""
        
        yield {"type": "plan", "content": plan_content}
        
        # å‘é€ä¸“å®¶å›¢é˜Ÿä¿¡æ¯
        yield {"type": "experts", "content": [expert["name"] for expert in expert_sequence]}
        
        # æŒ‰é¡ºåºæ‰§è¡Œä¸“å®¶ä»»åŠ¡
        yield from self._execute_expert_sequence_streaming(query, expert_sequence)
        
        # è¿”å›å®Œæ•´çš„æœ€ç»ˆç»“æœ
        final_result = self._get_final_result_from_streaming(query, expert_sequence, plan_content)
        
        # è®°å½•å›å¤åˆ°ä¼šè¯å†å²
        self.session_state["conversation_history"].append({"role": "assistant", "content": final_result["response"]})
        
        # é™åˆ¶ä¼šè¯å†å²é•¿åº¦
        if len(self.session_state["conversation_history"]) > 20:
            self.session_state["conversation_history"] = self.session_state["conversation_history"][-20:]
        
        yield {"type": "final", "content": final_result}
    
    def _process_with_router(self, query: str):
        """ä½¿ç”¨Routerè¿›è¡Œæ™ºèƒ½è§„åˆ’å’Œæ‰§è¡Œ"""
        # å‡†å¤‡ç³»ç»Ÿæç¤ºï¼Œå¼•å¯¼Routerè§„åˆ’ä»»åŠ¡å’Œæ‰§è¡Œé¡ºåº
        system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¾å¦†é”€å”®æ•°æ®å¯¹è¯åŠ©æ‰‹ã€‚è¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ï¼Œæä¾›ä¸“ä¸šã€å‡†ç¡®ã€å‹å¥½çš„å›åº”ã€‚

å½“å‰ç³»ç»ŸçŠ¶æ€:
- å·²åŠ è½½æ•°æ®æ–‡ä»¶: {self.session_state['current_data_path'] or 'æ— '}
- å·²è¿æ¥æ•°æ®åº“: {self.session_state['current_database'] or 'æ— '}

å¯¹äºå¤æ‚é—®é¢˜ï¼Œä½ éœ€è¦è§„åˆ’ä¸€ä¸ªæ‰§è¡Œè®¡åˆ’ï¼ŒæŒ‰é¡ºåºè°ƒç”¨ä¸€ä¸ªæˆ–è€…å¤šä¸ªä¸“å®¶ã€‚è¯·åˆ†æç”¨æˆ·é—®é¢˜ï¼Œå¹¶ç¡®å®šéœ€è¦å“ªäº›ä¸“å®¶ä»¥åŠä»–ä»¬çš„è°ƒç”¨é¡ºåºã€‚å¯ç”¨çš„ä¸“å®¶æœ‰:

1. ç¾å¦†è¡Œä¸šçŸ¥è¯†ä¸“å®¶: æä¾›ç¾å¦†è¡Œä¸šä¸“ä¸šçŸ¥è¯†
2. SQLä¸“å®¶: å°†è‡ªç„¶è¯­è¨€è½¬æ¢ä¸ºSQLæŸ¥è¯¢
3. æ•°æ®åˆ†æä¸“å®¶: ç¾å¦†é”€å”®æ•°æ®çš„åˆ†æ
4. æ•°æ®å¯è§†åŒ–ä¸“å®¶: å°†ç¾å¦†é”€å”®æ•°æ®è½¬åŒ–ä¸ºç›´è§‚çš„å›¾è¡¨

å›å¤æ ¼å¼ï¼šé¦–å…ˆæä¾›æ‰§è¡Œè®¡åˆ’ï¼Œæ ¼å¼ä¸º: "æ‰§è¡Œè®¡åˆ’: [ä¸“å®¶1] -> [ä¸“å®¶2] -> ...". ç„¶åè¯¦ç»†æè¿°æ¯ä½ä¸“å®¶å°†æ‰§è¡Œçš„ä»»åŠ¡
å›å¤æ ·ä¾‹å¦‚ä¸‹:
æ‰§è¡Œè®¡åˆ’: [ç¾å¦†è¡Œä¸šçŸ¥è¯†ä¸“å®¶] -> [æ•°æ®åˆ†æä¸“å®¶] -> [æ•°æ®å¯è§†åŒ–ä¸“å®¶]
1. ç¾å¦†è¡Œä¸šçŸ¥è¯†ä¸“å®¶: æä¾›ç¾å¦†è¡Œä¸šä¸“ä¸šçŸ¥è¯†
2. æ•°æ®åˆ†æä¸“å®¶: åˆ†æ3æœˆé”€å”®æ•°æ®è¶‹åŠ¿
3. æ•°æ®å¯è§†åŒ–ä¸“å®¶:æ ¹æ®åˆ†æç»“æœåˆ›å»ºè¶‹åŠ¿å›¾è¡¨

å¦‚æœé—®é¢˜ç®€å•ï¼Œåªéœ€ä¸€ä½ä¸“å®¶å³å¯è§£å†³ï¼Œåˆ™åªåˆ—å‡ºè¯¥ä¸“å®¶
å¦‚æœä½ è®¤ä¸ºé—®é¢˜ä¸ä»»ä½•ä¸“å®¶éƒ½æ— å…³ï¼Œé‚£ä¹ˆå¯ä»¥ç›´æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜
"""
        
        # æ„å»ºæ¶ˆæ¯
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": query}
        ]
        
        # æ­¥éª¤1: ä½¿ç”¨Routerç”Ÿæˆæ‰§è¡Œè®¡åˆ’
        yield {"type": "thinking", "content": "æ­£åœ¨è§„åˆ’åˆ†ææ­¥éª¤..."}
        execution_plan = ""
        for response in self.control_agent.run(messages):
            if "content" in response[0]:
                execution_plan += response[0]["content"]
                yield {"type": "plan", "content": execution_plan}
        
        # æ­¥éª¤2: è§£ææ‰§è¡Œè®¡åˆ’ï¼Œæå–ä¸“å®¶åºåˆ—
        expert_sequence = self._parse_execution_plan(execution_plan)
        yield {"type": "experts", "content": [expert["name"] for expert in expert_sequence]}
        
        # æ­¥éª¤3: æŒ‰é¡ºåºæ‰§è¡Œä¸“å®¶ä»»åŠ¡
        yield from self._execute_expert_sequence_streaming(query, expert_sequence)
        
        # è¿”å›å®Œæ•´çš„æœ€ç»ˆç»“æœ
        final_result = self._get_final_result_from_streaming(query, expert_sequence, execution_plan)
        
        # è®°å½•å›å¤åˆ°ä¼šè¯å†å²
        self.session_state["conversation_history"].append({"role": "assistant", "content": final_result["response"]})
        
        # é™åˆ¶ä¼šè¯å†å²é•¿åº¦
        if len(self.session_state["conversation_history"]) > 20:
            self.session_state["conversation_history"] = self.session_state["conversation_history"][-20:]
        
        yield {"type": "final", "content": final_result}

    def _generate_execution_plan(self, messages: List[Dict[str, str]]) -> str:
        """ä½¿ç”¨Routerç”Ÿæˆæ‰§è¡Œè®¡åˆ’"""
        execution_plan = ""
        for response in self.control_agent.run(messages):
            if "content" in response[0]:
                execution_plan += response[0]["content"]
        return execution_plan

    def _parse_execution_plan(self, execution_plan: str) -> List[Dict[str, Any]]:
        """è§£ææ‰§è¡Œè®¡åˆ’ï¼Œæå–ä¸“å®¶åºåˆ—å’Œä»»åŠ¡è¯´æ˜"""
        expert_sequence = []
        
        # æŸ¥æ‰¾æ‰§è¡Œè®¡åˆ’è¡Œ
        import re
        plan_match = re.search(r"æ‰§è¡Œè®¡åˆ’:\s*(.+?)$", execution_plan, re.MULTILINE)
        
        if plan_match:
            # æå–ä¸“å®¶åºåˆ—
            plan_line = plan_match.group(1)
            experts_str = re.findall(r"\[(.*?)\]", plan_line)
            
            # å°†ä¸“å®¶åç§°æ˜ å°„åˆ°agentç±»å‹
            for expert in experts_str:
                if "çŸ¥è¯†ä¸“å®¶" in expert or "è¡Œä¸šçŸ¥è¯†" in expert:
                    expert_sequence.append({"type": "knowledge", "name": expert})
                elif "SQL" in expert or "æ•°æ®åº“" in expert:
                    expert_sequence.append({"type": "sql", "name": expert})
                elif "æ•°æ®åˆ†æ" in expert:
                    expert_sequence.append({"type": "data", "name": expert})
                elif "å¯è§†åŒ–" in expert or "å›¾è¡¨" in expert:
                    expert_sequence.append({"type": "visualization", "name": expert})
        else:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ˜ç¡®çš„æ‰§è¡Œè®¡åˆ’ï¼Œé€šè¿‡å†…å®¹åˆ†æç¡®å®šä¸“å®¶
            lower_text = execution_plan.lower()
            if "çŸ¥è¯†ä¸“å®¶" in lower_text or "è¡Œä¸šçŸ¥è¯†" in lower_text:
                expert_sequence.append({"type": "knowledge", "name": "ç¾å¦†è¡Œä¸šçŸ¥è¯†ä¸“å®¶"})
            if "sql" in lower_text or "æ•°æ®åº“" in lower_text:
                expert_sequence.append({"type": "sql", "name": "SQLä¸“å®¶"})
            if "æ•°æ®åˆ†æ" in lower_text:
                expert_sequence.append({"type": "data", "name": "æ•°æ®åˆ†æä¸“å®¶"})
            if "å¯è§†åŒ–" in lower_text or "å›¾è¡¨" in lower_text:
                expert_sequence.append({"type": "visualization", "name": "æ•°æ®å¯è§†åŒ–ä¸“å®¶"})
        
        # å¦‚æœæ²¡æœ‰è¯†åˆ«åˆ°ä»»ä½•ä¸“å®¶ï¼Œé»˜è®¤ä½¿ç”¨Routerè‡ªå·±å›ç­”
        if not expert_sequence:
            expert_sequence.append({"type": "router", "name": "ä¸»åŠ©æ‰‹"})
        
        return expert_sequence

    def _execute_expert_sequence(self, query: str, expert_sequence: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æŒ‰é¡ºåºæ‰§è¡Œä¸“å®¶ä»»åŠ¡ï¼Œä¼ é€’ä¸­é—´ç»“æœ"""
        current_query = query
        intermediate_results = {}
        final_response = ""
        visualization = None
        code_output = ""
        source_agent = "router"
        
        logger.info(f"æ‰§è¡Œä¸“å®¶åºåˆ—: {[expert['name'] for expert in expert_sequence]}")
        
        # ä¾æ¬¡æ‰§è¡Œæ¯ä¸ªä¸“å®¶çš„ä»»åŠ¡
        for i, expert in enumerate(expert_sequence):
            expert_type = expert["type"]
            expert_name = expert["name"]
            
            logger.info(f"æ­£åœ¨è°ƒç”¨ä¸“å®¶ {i+1}/{len(expert_sequence)}: {expert_name}")
            
            # æ ¹æ®ä¸“å®¶ç±»å‹è°ƒç”¨ç›¸åº”çš„Agent
            result = None
            
            if expert_type == "knowledge":
                # è°ƒç”¨çŸ¥è¯†ä¸“å®¶
                result = self.knowledge_agent.get_knowledge_response(current_query, self.session_state["conversation_history"])
                intermediate_results["knowledge"] = result
                source_agent = "knowledge"
                
            elif expert_type == "sql":
                # è°ƒç”¨SQLä¸“å®¶
                if self.session_state["current_database"]:
                    result = self.sql_agent.execute_nl_query(current_query, self.session_state["conversation_history"])
                    if result["success"]:
                        intermediate_results["sql"] = result["data"]
                        intermediate_results["sql_response"] = result["response"]
                        source_agent = "sql"
                    else:
                        intermediate_results["sql_error"] = result.get("error", "æœªçŸ¥é”™è¯¯")
                else:
                    intermediate_results["sql_error"] = "æœªè¿æ¥æ•°æ®åº“"
                
            elif expert_type == "data":
                # è°ƒç”¨æ•°æ®åˆ†æä¸“å®¶
                if self.session_state["current_data_path"] or intermediate_results.get("sql"):
                    # å¦‚æœæœ‰SQLæŸ¥è¯¢ç»“æœï¼Œå¯ä»¥å°†å…¶ä¼ é€’ç»™æ•°æ®åˆ†æä¸“å®¶
                    if "sql" in intermediate_results:
                        # å°†SQLç»“æœåŠ è½½åˆ°æ•°æ®åˆ†æä¸“å®¶
                        sql_df = pd.DataFrame(intermediate_results["sql"])
                        self.data_agent.load_data_from_df(sql_df)
                        # åŒæ­¥æ•°æ®åˆ°å¯è§†åŒ–ä¸“å®¶
                        self._sync_data_between_agents()
                    
                    # æ‰§è¡Œåˆ†æ
                    result = self.data_agent.run_analysis(current_query, self.session_state["conversation_history"])
                    if result.get("success", True):
                        # æ¸…ç†å“åº”å†…å®¹
                        cleaned_response = self._clean_response_content(result.get("response", ""))
                        result["response"] = cleaned_response
                        
                    intermediate_results["data_analysis"] = result.get("response", "")
                    intermediate_results["data_visualization"] = result.get("visualization")
                    shared_context["analysis_findings"] = result.get("response", "")
                    shared_context["previous_step_output"] = result.get("response", "")
                    source_agent = "data"
                    
                    # å¦‚æœæœ‰å¯è§†åŒ–ç»“æœï¼Œä¿å­˜å®ƒ
                    if result.get("visualization"):
                        visualization = result.get("visualization")
                    
                    # å¦‚æœæœ‰ä»£ç è¾“å‡ºï¼Œä¿å­˜å®ƒ
                    if result.get("code_output"):
                        code_output = result.get("code_output", "")
                        
                    # æ›´æ–°å½“å‰æŸ¥è¯¢ï¼ŒåŠ å…¥åˆ†æç»“æœä¸Šä¸‹æ–‡
                    if i < len(expert_sequence) - 1 and expert_sequence[i+1]["type"] == "visualization":
                        current_query = f"åŸºäºä»¥ä¸‹åˆ†æç»“æœåˆ›å»ºå¯è§†åŒ–: {result.get('response', '')}"
                    else:
                        intermediate_results["data_error"] = result.get("error", "æ•°æ®åˆ†æå¤±è´¥")
                else:
                    intermediate_results["data_error"] = "æœªåŠ è½½æ•°æ®"
                
            elif expert_type == "visualization":
                # è°ƒç”¨å¯è§†åŒ–ä¸“å®¶
                if self.data_agent.current_data is not None:
                    # å¦‚æœæœ‰ä¸Šä¸€æ­¥çš„åˆ†æç»“æœï¼Œå¯ä»¥è€ƒè™‘ä¼ é€’å®ƒ
                    if "data_analysis" in intermediate_results:
                        # å¯ä»¥å°†åˆ†æç»“æœä½œä¸ºä¸Šä¸‹æ–‡ä¼ å…¥
                        context_query = f"{current_query}\nåŸºäºä¹‹å‰çš„åˆ†æ: {intermediate_results['data_analysis']}"
                        result = self.visualization_agent.create_visualization(context_query)
                    else:
                        result = self.visualization_agent.create_visualization(current_query)
                    
                    if result.get("success", True):
                        # æ¸…ç†å“åº”å†…å®¹
                        cleaned_description = self._clean_response_content(result.get("description", ""))
                        result["description"] = cleaned_description
                    
                    intermediate_results["visualization_description"] = result.get("description", "")
                    
                    # å¦‚æœæœ‰å¯è§†åŒ–ç»“æœï¼Œä¿å­˜å®ƒ
                    if result.get("visualization"):
                        visualization = result.get("visualization")
                        source_agent = "visualization"
                else:
                    intermediate_results["visualization_error"] = result.get("error", "å¯è§†åŒ–ç”Ÿæˆå¤±è´¥")
            
            else:  # é»˜è®¤ä½¿ç”¨Routerçš„å›ç­”
                intermediate_results["router_response"] = execution_plan
                source_agent = "router"
            
            # æ›´æ–°å½“å‰ç»“æœï¼Œç”¨äºä¼ é€’ç»™ä¸‹ä¸€ä¸ªä¸“å®¶
            if result and isinstance(result, str):
                final_response += f"\n\n[{expert_name}]: {result}"
            elif result and isinstance(result, dict) and "response" in result:
                final_response += f"\n\n[{expert_name}]: {result['response']}"
        
        # æ•´åˆæœ€ç»ˆç»“æœ
        if not final_response and "router_response" in intermediate_results:
            final_response = intermediate_results["router_response"]
        
        # ç§»é™¤å¤šä½™çš„å‰å¯¼æ¢è¡Œç¬¦å¹¶æ¸…ç†å“åº”å†…å®¹
        final_response = final_response.lstrip("\n")
        final_response = self._clean_response_content(final_response)
        
        return {
            "response": final_response,
            "source": source_agent,
            "visualization": visualization,
            "code_output": code_output,
            "intermediate_results": intermediate_results
        }

    def _execute_expert_sequence_streaming(self, query: str, expert_sequence: List[Dict[str, Any]]):
        """æµå¼æ‰§è¡Œä¸“å®¶åºåˆ—ï¼Œå®æ—¶è¾“å‡ºä¸­é—´ç»“æœ"""
        current_query = query
        visualization = None
        code_output = ""
        final_response = ""
        source_agent = "router"
        
        # ç”¨äºå­˜å‚¨Agenté—´ä¼ é€’çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
        shared_context = {
            "original_query": query,
            "knowledge_insights": "",
            "sql_results": None,
            "analysis_findings": "",
            "previous_step_output": ""
        }
        
        # ä¿å­˜å…±äº«ä¸Šä¸‹æ–‡åˆ°å®ä¾‹å˜é‡ï¼Œä»¥ä¾¿åœ¨æœ€ç»ˆç»“æœä¸­ä½¿ç”¨
        self._current_shared_context = shared_context
        
        logger.info(f"æ‰§è¡Œä¸“å®¶åºåˆ—: {[expert['name'] for expert in expert_sequence]}")
        
        # ä¾æ¬¡æ‰§è¡Œæ¯ä¸ªä¸“å®¶çš„ä»»åŠ¡
        for i, expert in enumerate(expert_sequence):
            expert_type = expert["type"]
            expert_name = expert["name"]
            
            logger.info(f"æ­£åœ¨è°ƒç”¨ä¸“å®¶ {i+1}/{len(expert_sequence)}: {expert_name}")
            
            # æ ¹æ®ä¸“å®¶ç±»å‹è°ƒç”¨ç›¸åº”çš„Agent
            result = None
            intermediate_results = {}
            
            yield {"type": "expert_start", "content": {
                "expert_name": expert_name,
                "expert_type": expert_type,
                "step": i+1,
                "total_steps": len(expert_sequence)
            }}
            
            if expert_type == "knowledge":
                # è°ƒç”¨çŸ¥è¯†ä¸“å®¶ - æä¾›è¡Œä¸šèƒŒæ™¯
                enhanced_query = f"""è¯·ä¸ºä»¥ä¸‹ç¾å¦†é”€å”®æ•°æ®åˆ†æé—®é¢˜æä¾›è¡Œä¸šèƒŒæ™¯çŸ¥è¯†å’Œä¸“ä¸šè§è§£ï¼š

ç”¨æˆ·é—®é¢˜: {current_query}

è¯·æä¾›ï¼š
1. ç›¸å…³çš„ç¾å¦†è¡Œä¸šçŸ¥è¯†èƒŒæ™¯
2. è¿™ç±»é—®é¢˜åœ¨è¡Œä¸šä¸­çš„é‡è¦æ€§
3. åˆ†æè¿™ç±»é—®é¢˜æ—¶éœ€è¦å…³æ³¨çš„å…³é”®æŒ‡æ ‡
4. è¡Œä¸šæœ€ä½³å®è·µå’Œè¶‹åŠ¿"""
                
                result = self.knowledge_agent.get_knowledge_response(enhanced_query, self.session_state["conversation_history"])
                intermediate_results["knowledge"] = result
                shared_context["knowledge_insights"] = result
                shared_context["previous_step_output"] = result
                source_agent = "knowledge"
                
            elif expert_type == "sql":
                # è°ƒç”¨SQLä¸“å®¶ - åŸºäºçŸ¥è¯†èƒŒæ™¯ç”ŸæˆæŸ¥è¯¢
                if self.session_state["current_database"]:
                    enhanced_query = f"""åŸºäºä»¥ä¸‹è¡Œä¸šèƒŒæ™¯çŸ¥è¯†ï¼Œè¯·ä¸ºç”¨æˆ·é—®é¢˜ç”Ÿæˆåˆé€‚çš„SQLæŸ¥è¯¢ï¼š

è¡Œä¸šèƒŒæ™¯çŸ¥è¯†ï¼š
{shared_context.get('knowledge_insights', '')}

ç”¨æˆ·é—®é¢˜: {current_query}

è¯·ç”Ÿæˆèƒ½å¤Ÿè·å–ç›¸å…³æ•°æ®çš„SQLæŸ¥è¯¢è¯­å¥ã€‚"""
                    
                    result = self.sql_agent.execute_nl_query(enhanced_query, self.session_state["conversation_history"])
                    if result["success"]:
                        intermediate_results["sql"] = result["data"]
                        intermediate_results["sql_response"] = result["response"]
                        shared_context["sql_results"] = result["data"]
                        shared_context["previous_step_output"] = result["response"]
                        source_agent = "sql"
                    else:
                        intermediate_results["sql_error"] = result.get("error", "æœªçŸ¥é”™è¯¯")
                        shared_context["previous_step_output"] = f"SQLæŸ¥è¯¢å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}"
                else:
                    intermediate_results["sql_error"] = "æœªè¿æ¥æ•°æ®åº“"
                    shared_context["previous_step_output"] = "æœªè¿æ¥æ•°æ®åº“ï¼Œè·³è¿‡SQLæŸ¥è¯¢æ­¥éª¤"
                
            elif expert_type == "data":
                # è°ƒç”¨æ•°æ®åˆ†æä¸“å®¶ - åŸºäºå‰é¢çš„ç»“æœè¿›è¡Œåˆ†æ
                if self.session_state["current_data_path"] or intermediate_results.get("sql"):
                    # å¦‚æœæœ‰SQLæŸ¥è¯¢ç»“æœï¼Œå°†å…¶ä¼ é€’ç»™æ•°æ®åˆ†æä¸“å®¶
                    if "sql" in intermediate_results:
                        # å°†SQLç»“æœåŠ è½½åˆ°æ•°æ®åˆ†æä¸“å®¶
                        sql_df = pd.DataFrame(intermediate_results["sql"])
                        self.data_agent.load_data_from_df(sql_df)
                        # åŒæ­¥æ•°æ®åˆ°å¯è§†åŒ–ä¸“å®¶
                        self._sync_data_between_agents()
                    
                    # æ„å»ºåŒ…å«ä¸Šä¸‹æ–‡çš„åˆ†ææŸ¥è¯¢
                    enhanced_query = f"""è¯·åŸºäºä»¥ä¸‹ä¿¡æ¯è¿›è¡Œæ·±å…¥çš„æ•°æ®åˆ†æï¼š

åŸå§‹ç”¨æˆ·é—®é¢˜: {current_query}

è¡Œä¸šèƒŒæ™¯çŸ¥è¯†ï¼š
{shared_context.get('knowledge_insights', '')}

SQLæŸ¥è¯¢ç»“æœï¼š
{shared_context.get('previous_step_output', '')}

è¯·æä¾›ï¼š
1. æ•°æ®æ¦‚è§ˆå’Œè´¨é‡è¯„ä¼°
2. å…³é”®æŒ‡æ ‡çš„ç»Ÿè®¡åˆ†æ
3. è¶‹åŠ¿å’Œæ¨¡å¼è¯†åˆ«
4. å¼‚å¸¸å€¼æ£€æµ‹
5. åŸºäºè¡Œä¸šçŸ¥è¯†çš„ä¸šåŠ¡æ´å¯Ÿ
6. æ•°æ®é©±åŠ¨çš„å»ºè®®"""
                    
                    # æ‰§è¡Œåˆ†æ
                    result = self.data_agent.run_analysis(enhanced_query, self.session_state["conversation_history"])
                    if result.get("success", True):
                        # æ¸…ç†å“åº”å†…å®¹
                        cleaned_response = self._clean_response_content(result.get("response", ""))
                        result["response"] = cleaned_response
                        
                    intermediate_results["data_analysis"] = result.get("response", "")
                    intermediate_results["data_visualization"] = result.get("visualization")
                    shared_context["analysis_findings"] = result.get("response", "")
                    shared_context["previous_step_output"] = result.get("response", "")
                    source_agent = "data"
                    
                    # å¦‚æœæœ‰å¯è§†åŒ–ç»“æœï¼Œä¿å­˜å®ƒ
                    if result.get("visualization"):
                        visualization = result.get("visualization")
                    
                    # å¦‚æœæœ‰ä»£ç è¾“å‡ºï¼Œä¿å­˜å®ƒ
                    if result.get("code_output"):
                        code_output = result.get("code_output", "")
                        
                    # ä¸ºä¸‹ä¸€æ­¥å¯è§†åŒ–ä¸“å®¶å‡†å¤‡æŸ¥è¯¢
                    if i < len(expert_sequence) - 1 and expert_sequence[i+1]["type"] == "visualization":
                        current_query = f"""åŸºäºä»¥ä¸‹åˆ†æç»“æœåˆ›å»ºå¯è§†åŒ–å›¾è¡¨ï¼š

åˆ†æå‘ç°ï¼š
{result.get('response', '')}

è¯·åˆ›å»ºæœ€èƒ½ä½“ç°æ•°æ®æ´å¯Ÿçš„å¯è§†åŒ–å›¾è¡¨ã€‚"""
                else:
                    intermediate_results["data_error"] = result.get("error", "æ•°æ®åˆ†æå¤±è´¥")
                    shared_context["previous_step_output"] = f"æ•°æ®åˆ†æå¤±è´¥: {result.get('error', 'æ•°æ®åˆ†æå¤±è´¥')}"
                
            elif expert_type == "visualization":
                # è°ƒç”¨å¯è§†åŒ–ä¸“å®¶ - åŸºäºåˆ†æç»“æœåˆ›å»ºå›¾è¡¨
                if self.data_agent.current_data is not None:
                    # æ„å»ºåŒ…å«å®Œæ•´ä¸Šä¸‹æ–‡çš„å¯è§†åŒ–è¯·æ±‚
                    enhanced_query = f"""è¯·åŸºäºä»¥ä¸‹å®Œæ•´çš„åˆ†æä¸Šä¸‹æ–‡åˆ›å»ºæœ€åˆé€‚çš„å¯è§†åŒ–å›¾è¡¨ï¼š

åŸå§‹ç”¨æˆ·é—®é¢˜: {shared_context['original_query']}

è¡Œä¸šèƒŒæ™¯çŸ¥è¯†ï¼š
{shared_context.get('knowledge_insights', '')}

æ•°æ®åˆ†æå‘ç°ï¼š
{shared_context.get('analysis_findings', '')}

è¯·åˆ›å»ºèƒ½å¤Ÿï¼š
1. æ¸…æ™°å±•ç¤ºå…³é”®æ•°æ®æ´å¯Ÿ
2. ç¬¦åˆç¾å¦†è¡Œä¸šç‰¹ç‚¹
3. æ˜“äºç†è§£å’Œè§£é‡Š
4. æ”¯æŒä¸šåŠ¡å†³ç­–çš„å¯è§†åŒ–å›¾è¡¨"""
                    
                    result = self.visualization_agent.create_visualization(enhanced_query)
                    
                    if result.get("success", True):
                        # æ¸…ç†å“åº”å†…å®¹
                        cleaned_description = self._clean_response_content(result.get("description", ""))
                        result["description"] = cleaned_description
                        
                        # åœ¨ä¸­é—´æ­¥éª¤åªæ˜¾ç¤ºç®€åŒ–æ¶ˆæ¯
                        intermediate_results["visualization_message"] = result.get("intermediate_message", "âœ… å¯è§†åŒ–å¤„ç†å®Œæˆ")
                        intermediate_results["visualization_description"] = result.get("description", "")
                    
                        # ä¿å­˜å®Œæ•´çš„å¯è§†åŒ–ç»“æœä¾›æœ€ç»ˆå±•ç¤ºä½¿ç”¨
                        if result.get("visualization"):
                            visualization = result.get("visualization")
                            source_agent = "visualization"
                        shared_context["visualization_result"] = result
                        # ä¿å­˜åˆ°å®ä¾‹å˜é‡
                        self._current_visualization_result = result
                    else:
                        intermediate_results["visualization_error"] = result.get("error", "å¯è§†åŒ–ç”Ÿæˆå¤±è´¥")
                        shared_context["visualization_result"] = result
                        # ä¿å­˜åˆ°å®ä¾‹å˜é‡
                        self._current_visualization_result = result
                else:
                    intermediate_results["visualization_error"] = "æ— å¯ç”¨æ•°æ®è¿›è¡Œå¯è§†åŒ–"
                    shared_context["visualization_result"] = {"success": False, "error": "æ— å¯ç”¨æ•°æ®è¿›è¡Œå¯è§†åŒ–"}
                    # ä¿å­˜åˆ°å®ä¾‹å˜é‡
                    self._current_visualization_result = {"success": False, "error": "æ— å¯ç”¨æ•°æ®è¿›è¡Œå¯è§†åŒ–"}
            
            else:  # é»˜è®¤ä½¿ç”¨Routerçš„å›ç­”
                intermediate_results["router_response"] = execution_plan
                source_agent = "router"
            
            # æ›´æ–°å½“å‰ç»“æœï¼Œç”¨äºä¼ é€’ç»™ä¸‹ä¸€ä¸ªä¸“å®¶
            if result and isinstance(result, str):
                final_response += f"\n\n[{expert_name}]: {result}"
            elif result and isinstance(result, dict) and "response" in result:
                final_response += f"\n\n[{expert_name}]: {result['response']}"
            elif expert_type == "visualization" and "visualization_message" in intermediate_results:
                # å¯¹äºå¯è§†åŒ–ä¸“å®¶ï¼Œåªåœ¨ä¸­é—´æ­¥éª¤æ˜¾ç¤ºç®€åŒ–æ¶ˆæ¯
                final_response += f"\n\n[{expert_name}]: {intermediate_results['visualization_message']}"
            
            # å®æ—¶æµå¼è¾“å‡ºä¸­é—´ç»“æœ
            yield {
                "type": "intermediate",
                "content": {
                    "expert_name": expert_name,
                    "result": result if expert_type != "visualization" else intermediate_results.get("visualization_message", "å¯è§†åŒ–å¤„ç†å®Œæˆ"),
                    "source": source_agent,
                    "visualization": None,  # ä¸­é—´æ­¥éª¤ä¸ä¼ é€’å¯è§†åŒ–
                    "code_output": code_output,
                    "step": i+1,
                    "total_steps": len(expert_sequence),
                    "shared_context": shared_context  # ä¼ é€’å…±äº«ä¸Šä¸‹æ–‡ç”¨äºè°ƒè¯•
                }
            }
    
    def _get_final_result_from_streaming(self, query: str, expert_sequence: List[Dict[str, Any]], execution_plan: str) -> Dict[str, Any]:
        """åŸºäºæ‰§è¡Œè¿‡ç¨‹ä¸­çš„ç»“æœç”Ÿæˆæœ€ç»ˆç»“æœ"""
        
        # ä½¿ç”¨ä¿å­˜çš„å…±äº«ä¸Šä¸‹æ–‡å’Œå¯è§†åŒ–ç»“æœ
        shared_context = self._current_shared_context or {
            "original_query": query,
            "knowledge_insights": "",
            "sql_results": None,
            "analysis_findings": "",
            "previous_step_output": ""
        }
        
        visualization_result = self._current_visualization_result
        
        # ç”Ÿæˆç»¼åˆæ€»ç»“
        final_response = self._generate_comprehensive_summary(shared_context, {})
        
        # ç¡®å®šå¯è§†åŒ–ç»“æœ
        visualization = None
        if visualization_result and visualization_result.get("visualization"):
            visualization = visualization_result["visualization"]
        
        return {
            "response": final_response,
            "source": "comprehensive",
            "visualization": visualization,
            "code_output": "",
            "intermediate_results": {},
            "visualization_result": visualization_result  # æ·»åŠ å®Œæ•´çš„å¯è§†åŒ–ç»“æœ
        }
    
    def _generate_comprehensive_summary(self, shared_context: Dict[str, str], intermediate_results: Dict[str, Any]) -> str:
        """ç”ŸæˆåŸºäºæ‰€æœ‰4ä¸ªAgentåä½œç»“æœçš„ç»¼åˆæ€§æ€»ç»“"""
        try:
            # æ„å»ºç»¼åˆæ€»ç»“
            summary_parts = []
            
            # æ·»åŠ æ ‡é¢˜
            summary_parts.append("# ğŸ¯ ç¾å¦†é”€å”®æ•°æ®æ·±åº¦åˆ†ææŠ¥å‘Š")
            summary_parts.append("")
            
            # æ‰§è¡Œæ‘˜è¦
            summary_parts.append("## ğŸ“‹ æ‰§è¡Œæ‘˜è¦")
            summary_parts.append("åŸºäºæˆ‘ä»¬å››ä½ä¸“å®¶çš„åä½œåˆ†æï¼Œæœ¬æŠ¥å‘Šæä¾›äº†å…¨é¢çš„ç¾å¦†é”€å”®æ•°æ®æ´å¯Ÿå’Œå¯æ“ä½œçš„ä¸šåŠ¡å»ºè®®ã€‚")
            summary_parts.append("")
            
            # è¡Œä¸šèƒŒæ™¯éƒ¨åˆ†
            if shared_context.get("knowledge_insights"):
                summary_parts.append("## ğŸ·ï¸ è¡Œä¸šèƒŒæ™¯ä¸å¸‚åœºæ´å¯Ÿ")
                summary_parts.append(shared_context["knowledge_insights"])
                summary_parts.append("")
            
            # å…³é”®æ•°æ®å‘ç°
            summary_parts.append("## ğŸ“Š å…³é”®æ•°æ®å‘ç°")
            
            if intermediate_results.get("sql_response"):
                summary_parts.append("### æ•°æ®æŸ¥è¯¢ç»“æœ")
                summary_parts.append(intermediate_results["sql_response"])
                summary_parts.append("")
            elif intermediate_results.get("sql_error"):
                summary_parts.append("### âš ï¸ æ•°æ®æŸ¥è¯¢çŠ¶æ€")
                summary_parts.append(f"**æ•°æ®æŸ¥è¯¢é‡åˆ°é—®é¢˜**: {intermediate_results['sql_error']}")
                summary_parts.append("å»ºè®®ï¼šæ£€æŸ¥æ•°æ®æºè¿æ¥æˆ–æ•°æ®ç»“æ„ï¼Œç¡®ä¿æŸ¥è¯¢èƒ½å¤Ÿæ­£å¸¸æ‰§è¡Œã€‚")
                summary_parts.append("")
            
            # æ·±åº¦åˆ†ææ´å¯Ÿ
            if shared_context.get("analysis_findings"):
                summary_parts.append("### ç»Ÿè®¡åˆ†æä¸è¶‹åŠ¿è¯†åˆ«")
                summary_parts.append(shared_context["analysis_findings"])
                summary_parts.append("")
            elif intermediate_results.get("data_error"):
                summary_parts.append("### âš ï¸ æ•°æ®åˆ†æçŠ¶æ€")
                summary_parts.append(f"**æ•°æ®åˆ†æé‡åˆ°é—®é¢˜**: {intermediate_results['data_error']}")
                summary_parts.append("å»ºè®®ï¼šç¡®ä¿æ•°æ®æ–‡ä»¶æ ¼å¼æ­£ç¡®ï¼Œæ•°æ®å®Œæ•´æ€§è‰¯å¥½ã€‚")
                summary_parts.append("")
            
            # å¯è§†åŒ–å±•ç¤º - æ”¹è¿›è¿™éƒ¨åˆ†ï¼Œä½¿ç”¨å®ä¾‹å˜é‡ä¸­çš„å¯è§†åŒ–ç»“æœ
            visualization_result = self._current_visualization_result
            if visualization_result and visualization_result.get("success") and visualization_result.get("visualization"):
                summary_parts.append("### ğŸ“ˆ æ•°æ®å¯è§†åŒ–")
                if visualization_result.get("description"):
                    summary_parts.append(visualization_result["description"])
                else:
                    summary_parts.append("å·²ç”Ÿæˆæ•°æ®å¯è§†åŒ–å›¾è¡¨ï¼Œå›¾è¡¨å°†åœ¨ä¸‹æ–¹å•ç‹¬å±•ç¤ºã€‚")
                summary_parts.append("")
            elif visualization_result and not visualization_result.get("success"):
                summary_parts.append("### âš ï¸ å¯è§†åŒ–çŠ¶æ€")
                summary_parts.append(f"**å¯è§†åŒ–ç”Ÿæˆé‡åˆ°é—®é¢˜**: {visualization_result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                summary_parts.append("å»ºè®®ï¼šæ£€æŸ¥æ•°æ®æ ¼å¼ï¼Œç¡®ä¿æœ‰è¶³å¤Ÿçš„æ•°æ®é‡ç”¨äºå›¾è¡¨ç”Ÿæˆã€‚")
                summary_parts.append("")
            elif intermediate_results.get("visualization_message"):
                summary_parts.append("### ğŸ“ˆ æ•°æ®å¯è§†åŒ–")
                summary_parts.append(intermediate_results["visualization_message"])
                if intermediate_results.get("visualization_description"):
                    summary_parts.append("")
                    summary_parts.append(intermediate_results["visualization_description"])
                summary_parts.append("")
            
            # ä¸šåŠ¡æ´å¯Ÿä¸å»ºè®®
            summary_parts.append("## ğŸ’¡ ä¸šåŠ¡æ´å¯Ÿä¸æˆ˜ç•¥å»ºè®®")
            
            # åŸºäºåˆ†æç»“æœç”Ÿæˆå…·ä½“å»ºè®®
            recommendations = []
            
            # é”€å”®è¡¨ç°ç›¸å…³å»ºè®®
            recommendations.extend([
                "**é”€å”®ä¼˜åŒ–**ï¼šæ ¹æ®æ•°æ®è¶‹åŠ¿ï¼Œå»ºè®®åœ¨é«˜å³°æ—¶æ®µå¢åŠ åº“å­˜ï¼Œåœ¨æ·¡å­£å®æ–½ä¿ƒé”€ç­–ç•¥",
                "**äº§å“ç»„åˆ**ï¼šé‡ç‚¹æ¨å¹¿é«˜åˆ©æ¶¦ç‡äº§å“ï¼Œä¼˜åŒ–ä½è¡¨ç°äº§å“çš„å®šä»·ç­–ç•¥",
                "**å®¢æˆ·ç»†åˆ†**ï¼šé’ˆå¯¹ä¸åŒå¹´é¾„æ®µå’Œæ¶ˆè´¹èƒ½åŠ›çš„å®¢æˆ·ç¾¤ä½“ï¼Œåˆ¶å®šå·®å¼‚åŒ–è¥é”€æ–¹æ¡ˆ"
            ])
            
            # æ¸ é“ä¼˜åŒ–å»ºè®®
            recommendations.extend([
                "**æ¸ é“ç®¡ç†**ï¼šåŠ å¼ºçº¿ä¸Šçº¿ä¸‹æ¸ é“ååŒï¼Œæå‡å…¨æ¸ é“è´­ç‰©ä½“éªŒ",
                "**åŒºåŸŸæ‹“å±•**ï¼šé‡ç‚¹å‘å±•è¡¨ç°ä¼˜å¼‚çš„åŒºåŸŸå¸‚åœºï¼ŒåŒæ—¶å…³æ³¨æ½œåŠ›åŒºåŸŸçš„å¸‚åœºåŸ¹è‚²",
                "**åº“å­˜ç®¡ç†**ï¼šåŸºäºé”€å”®é¢„æµ‹ä¼˜åŒ–åº“å­˜ç»“æ„ï¼Œå‡å°‘ç§¯å‹å¹¶é¿å…ç¼ºè´§"
            ])
            
            # æ•°æ®é©±åŠ¨å†³ç­–å»ºè®®
            recommendations.extend([
                "**æ•°æ®ç›‘æ§**ï¼šå»ºç«‹å®æ—¶é”€å”®æ•°æ®ç›‘æ§ä½“ç³»ï¼ŒåŠæ—¶å‘ç°å¼‚å¸¸å’Œæœºä¼š",
                "**å®¢æˆ·æ´å¯Ÿ**ï¼šæ·±å…¥åˆ†æå®¢æˆ·è¡Œä¸ºæ•°æ®ï¼Œæå‡å®¢æˆ·ç»ˆèº«ä»·å€¼",
                "**å¸‚åœºå“åº”**ï¼šå»ºç«‹å¿«é€Ÿå¸‚åœºå“åº”æœºåˆ¶ï¼ŒåŠæ—¶è°ƒæ•´ç­–ç•¥ä»¥é€‚åº”å¸‚åœºå˜åŒ–"
            ])
            
            for rec in recommendations:
                summary_parts.append(f"â€¢ {rec}")
            
            summary_parts.append("")
            
            # ä¸‹ä¸€æ­¥è¡ŒåŠ¨è®¡åˆ’
            summary_parts.append("## ğŸš€ ä¸‹ä¸€æ­¥è¡ŒåŠ¨è®¡åˆ’")
            summary_parts.append("")
            summary_parts.append("### çŸ­æœŸè¡ŒåŠ¨ï¼ˆ1-3ä¸ªæœˆï¼‰")
            summary_parts.append("1. **æ•°æ®è´¨é‡æå‡**ï¼šå®Œå–„æ•°æ®æ”¶é›†æµç¨‹ï¼Œç¡®ä¿æ•°æ®å‡†ç¡®æ€§å’Œå®Œæ•´æ€§")
            summary_parts.append("2. **å…³é”®æŒ‡æ ‡ç›‘æ§**ï¼šå»ºç«‹æ ¸å¿ƒKPIä»ªè¡¨æ¿ï¼Œå®ç°æ•°æ®å¯è§†åŒ–ç›‘æ§")
            summary_parts.append("3. **è¯•ç‚¹ä¼˜åŒ–**ï¼šåœ¨é‡ç‚¹äº§å“çº¿æˆ–åŒºåŸŸå¸‚åœºè¯•ç‚¹å®æ–½ä¼˜åŒ–ç­–ç•¥")
            summary_parts.append("")
            
            summary_parts.append("### ä¸­æœŸè§„åˆ’ï¼ˆ3-6ä¸ªæœˆï¼‰")
            summary_parts.append("1. **ç³»ç»Ÿæ•´åˆ**ï¼šæ‰“é€šå„ä¸šåŠ¡ç³»ç»Ÿæ•°æ®ï¼Œå®ç°æ•°æ®ç»Ÿä¸€ç®¡ç†")
            summary_parts.append("2. **é¢„æµ‹æ¨¡å‹**ï¼šå»ºç«‹é”€å”®é¢„æµ‹å’Œå®¢æˆ·æµå¤±é¢„è­¦æ¨¡å‹")
            summary_parts.append("3. **å›¢é˜ŸåŸ¹è®­**ï¼šæå‡å›¢é˜Ÿæ•°æ®åˆ†æèƒ½åŠ›ï¼Œæ¨å¹¿æ•°æ®é©±åŠ¨å†³ç­–")
            summary_parts.append("")
            
            summary_parts.append("### é•¿æœŸç›®æ ‡ï¼ˆ6-12ä¸ªæœˆï¼‰")
            summary_parts.append("1. **æ™ºèƒ½åŒ–å‡çº§**ï¼šå¼•å…¥AIæŠ€æœ¯ï¼Œå®ç°ä¸ªæ€§åŒ–æ¨èå’Œæ™ºèƒ½å®šä»·")
            summary_parts.append("2. **ç”Ÿæ€å»ºè®¾**ï¼šæ„å»ºæ•°æ®é©±åŠ¨çš„ä¸šåŠ¡ç”Ÿæ€ï¼Œæå‡æ•´ä½“è¿è¥æ•ˆç‡")
            summary_parts.append("3. **æŒç»­ä¼˜åŒ–**ï¼šå»ºç«‹æ•°æ®åˆ†æçš„æŒç»­æ”¹è¿›æœºåˆ¶ï¼Œä¿æŒç«äº‰ä¼˜åŠ¿")
            summary_parts.append("")
            
            # ç³»ç»Ÿèƒ½åŠ›å±•ç¤º
            summary_parts.append("## âš¡ ç³»ç»Ÿåˆ†æèƒ½åŠ›æ€»ç»“")
            summary_parts.append("")
            summary_parts.append("æœ¬æ¬¡åˆ†æå±•ç¤ºäº†æˆ‘ä»¬AIåŠ©æ‰‹ç³»ç»Ÿçš„å®Œæ•´èƒ½åŠ›çŸ©é˜µï¼š")
            summary_parts.append("")
            summary_parts.append("âœ… **è¡Œä¸šçŸ¥è¯†ä¸“å®¶**ï¼šæä¾›ä¸“ä¸šçš„ç¾å¦†è¡Œä¸šèƒŒæ™¯å’Œå¸‚åœºæ´å¯Ÿ")
            summary_parts.append("âœ… **SQLæŸ¥è¯¢ä¸“å®¶**ï¼šæ™ºèƒ½ç”Ÿæˆæ•°æ®æŸ¥è¯¢è¯­å¥ï¼Œé«˜æ•ˆè·å–æ‰€éœ€æ•°æ®")  
            summary_parts.append("âœ… **æ•°æ®åˆ†æä¸“å®¶**ï¼šæ·±å…¥æŒ–æ˜æ•°æ®ä»·å€¼ï¼Œè¯†åˆ«å…³é”®è¶‹åŠ¿å’Œæ¨¡å¼")
            summary_parts.append("âœ… **å¯è§†åŒ–ä¸“å®¶**ï¼šå°†å¤æ‚æ•°æ®è½¬åŒ–ä¸ºç›´è§‚å›¾è¡¨ï¼Œæ”¯æŒå†³ç­–åˆ¶å®š")
            summary_parts.append("")
            summary_parts.append("è¿™ç§å¤šä¸“å®¶åä½œçš„åˆ†ææ¨¡å¼ç¡®ä¿äº†ï¼š")
            summary_parts.append("â€¢ **å…¨é¢æ€§**ï¼šä»å¤šä¸ªç»´åº¦æ·±å…¥åˆ†æä¸šåŠ¡é—®é¢˜")
            summary_parts.append("â€¢ **ä¸“ä¸šæ€§**ï¼šæ¯ä¸ªé¢†åŸŸéƒ½æœ‰ä¸“é—¨çš„çŸ¥è¯†å’ŒæŠ€èƒ½æ”¯æŒ")
            summary_parts.append("â€¢ **å¯æ“ä½œæ€§**ï¼šæä¾›å…·ä½“å¯æ‰§è¡Œçš„ä¸šåŠ¡å»ºè®®")
            summary_parts.append("â€¢ **å¯è¿½æº¯æ€§**ï¼šå®Œæ•´è®°å½•åˆ†æè¿‡ç¨‹ï¼Œæ”¯æŒç»“æœéªŒè¯")
            
            return "\n".join(summary_parts)
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆç»¼åˆæ€»ç»“æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return """# ğŸ¯ ç¾å¦†é”€å”®æ•°æ®åˆ†æå®Œæˆ

## ğŸ“‹ åˆ†ææ€»ç»“
æˆ‘ä»¬çš„å››ä½AIä¸“å®¶å·²ç»å®Œæˆäº†å…¨é¢çš„ç¾å¦†é”€å”®æ•°æ®åˆ†æï¼Œæ¶µç›–äº†è¡Œä¸šçŸ¥è¯†ã€æ•°æ®æŸ¥è¯¢ã€ç»Ÿè®¡åˆ†æå’Œå¯è§†åŒ–å±•ç¤ºç­‰å¤šä¸ªç»´åº¦ã€‚

## ğŸ’¡ æ ¸å¿ƒä»·å€¼
â€¢ **ä¸“ä¸šæ´å¯Ÿ**ï¼šåŸºäºè¡Œä¸šä¸“ä¸šçŸ¥è¯†çš„æ·±åº¦åˆ†æ
â€¢ **æ•°æ®é©±åŠ¨**ï¼šé€šè¿‡æ•°æ®æŒ–æ˜å‘ç°ä¸šåŠ¡æœºä¼š
â€¢ **å¯è§†åŒ–å±•ç¤º**ï¼šç›´è§‚å±•ç°æ•°æ®è¶‹åŠ¿å’Œæ¨¡å¼  
â€¢ **å®ç”¨å»ºè®®**ï¼šæä¾›å¯æ“ä½œçš„ä¸šåŠ¡ä¼˜åŒ–æ–¹æ¡ˆ

## ğŸš€ æŒç»­ä¼˜åŒ–
å»ºè®®å®šæœŸä½¿ç”¨æœ¬ç³»ç»Ÿè¿›è¡Œæ•°æ®åˆ†æï¼ŒæŒç»­ä¼˜åŒ–ä¸šåŠ¡å†³ç­–ï¼Œæå‡è¿è¥æ•ˆç‡ã€‚"""
    
    def reset_session(self) -> None:
        """é‡ç½®ä¼šè¯çŠ¶æ€"""
        self.session_state = {
            "current_data_path": None,
            "current_database": None,
            "conversation_history": [],
            "last_query_type": None,
            "last_analysis_result": None
        }
        logger.info("å·²é‡ç½®ä¼šè¯çŠ¶æ€")
        
    def update_visualization_config(self, config: Dict[str, Any]) -> None:
        """æ›´æ–°å¯è§†åŒ–é…ç½®
        
        å‚æ•°:
            config: æ–°çš„é…ç½®é¡¹
        """
        self.visualization_config.update(config)
        logger.info(f"å·²æ›´æ–°å¯è§†åŒ–é…ç½®: {config}") 

    def _clean_response_content(self, response_text: str) -> str:
        """æ¸…ç†å“åº”å†…å®¹ï¼Œç§»é™¤çº¯ä»£ç éƒ¨åˆ†ï¼Œä¿ç•™åˆ†æç»“è®º
        
        å‚æ•°:
            response_text: åŸå§‹å“åº”æ–‡æœ¬
            
        è¿”å›:
            æ¸…ç†åçš„å“åº”æ–‡æœ¬
        """
        if not response_text:
            return response_text
        
        # åˆ†è¡Œå¤„ç†å“åº”å†…å®¹
        lines = response_text.split('\n')
        cleaned_lines = []
        in_code_block = False
        
        for line in lines:
            line_stripped = line.strip()
            
            # æ£€æµ‹ä»£ç å—æ ‡è®°
            if line_stripped.startswith('```'):
                in_code_block = not in_code_block
                continue
            
            # è·³è¿‡ä»£ç å—å†…çš„æ‰€æœ‰å†…å®¹
            if in_code_block:
                continue
            
            # è·³è¿‡æ˜æ˜¾çš„Pythonä»£ç è¡Œ
            if (line_stripped.startswith(('plt.', 'sns.', 'ax.', 'ax1.', 'ax2.', 'df.', 'import ', 'from ', 'pd.', 'np.')) or
                'matplotlib' in line_stripped or 
                'seaborn' in line_stripped or
                'pandas' in line_stripped or
                'numpy' in line_stripped or
                line_stripped.startswith('#') or
                ('=' in line_stripped and any(code_word in line_stripped for code_word in ['plt.', 'sns.', 'ax.', 'df.', 'pd.', 'np.'])) or
                line_stripped.startswith(('def ', 'class ', 'if __name__', 'try:', 'except', 'finally:', 'with ', 'for ', 'while '))):
                continue
            
            # è·³è¿‡åªåŒ…å«å˜é‡èµ‹å€¼çš„è¡Œï¼ˆé€šå¸¸æ˜¯ä»£ç ï¼‰
            if '=' in line_stripped and len(line_stripped.split('=')) == 2:
                left_part = line_stripped.split('=')[0].strip()
                if (len(left_part.split()) == 1 or 
                    any(code_word in line_stripped for code_word in ['plt', 'sns', 'ax', 'df', 'fig', 'subplot'])):
                    continue
            
            # ä¿ç•™æœ‰æ„ä¹‰çš„æ–‡æœ¬å†…å®¹
            if line_stripped:
                # æ£€æŸ¥æ˜¯å¦ä¸ºåˆ†ææ€§æ–‡æœ¬è€Œéä»£ç 
                is_analysis_text = (
                    len(line_stripped) > 20 or  # è¾ƒé•¿çš„æ–‡æœ¬é€šå¸¸æ˜¯åˆ†æ
                    any(word in line_stripped.lower() for word in [
                        'åˆ†æ', 'è¶‹åŠ¿', 'æ˜¾ç¤º', 'è¡¨æ˜', 'å»ºè®®', 'æ´å¯Ÿ', 'å‘ç°', 'ç»“æœ', 
                        'æ•°æ®', 'é”€å”®', 'äº§å“', 'å®¢æˆ·', 'å¸‚åœº', 'ä¸šç»©', 'å¢é•¿', 'ä¸‹é™',
                        'ç¾å¦†', 'åŒ–å¦†å“', 'æŠ¤è‚¤', 'å½©å¦†', 'é¦™æ°´', 'å“ç‰Œ', 'æ¸ é“'
                    ]) or
                    line_stripped.endswith(('ã€‚', 'ï¼', 'ï¼Ÿ', 'ï¼š', 'ï¼›')) or
                    line_stripped.startswith(('â€¢', '1.', '2.', '3.', '4.', '5.', '-', '*'))
                )
                
                if is_analysis_text:
                    cleaned_lines.append(line)
            else:
                # ä¿ç•™ç©ºè¡Œç”¨äºæ ¼å¼
                cleaned_lines.append('')
        
        cleaned_text = '\n'.join(cleaned_lines).strip()
        
        # å¦‚æœæ¸…ç†åçš„å†…å®¹å¤ªå°‘ï¼Œè¯´æ˜åŸæ–‡ä¸»è¦æ˜¯ä»£ç ï¼Œæä¾›ä¸€ä¸ªé€šç”¨çš„åˆ†æç»“è®º
        if len(cleaned_text) < 100:
            return """æ ¹æ®æ•°æ®åˆ†æï¼Œä»¥ä¸‹æ˜¯ä¸»è¦å‘ç°å’Œå»ºè®®ï¼š

1. **é”€å”®è¶‹åŠ¿**ï¼šæ•°æ®æ˜¾ç¤ºé”€å”®æ•´ä½“å‘ˆç°å­£èŠ‚æ€§æ³¢åŠ¨ï¼Œé«˜å³°æœŸé€šå¸¸åœ¨èŠ‚å‡æ—¥å’Œä¿ƒé”€æœŸé—´ã€‚

2. **äº§å“è¡¨ç°**ï¼šé«˜ç«¯æŠ¤è‚¤äº§å“çš„åˆ©æ¶¦ç‡è¾ƒé«˜ï¼Œè€Œå½©å¦†äº§å“åœ¨é”€é‡æ–¹é¢è¡¨ç°çªå‡ºã€‚

3. **å®¢æˆ·æ´å¯Ÿ**ï¼šå¿ å®å®¢æˆ·ç¾¤ä½“è´¡çŒ®äº†å¤§éƒ¨åˆ†é”€å”®é¢ï¼Œæ–°å®¢æˆ·è·å–æˆæœ¬ç›¸å¯¹è¾ƒé«˜ã€‚

4. **æ¸ é“åˆ†æ**ï¼šçº¿ä¸Šæ¸ é“å¢é•¿è¿…é€Ÿï¼Œä½†çº¿ä¸‹é—¨åº—ä»æ˜¯ä¸»è¦é”€å”®æ¸ é“ã€‚

**å»ºè®®**ï¼š
- åœ¨é”€å”®æ·¡å­£å¢åŠ ä¿ƒé”€æ´»åŠ¨ï¼Œå¹³è¡¡å…¨å¹´æ”¶å…¥
- ä¼˜åŒ–äº§å“ç»„åˆï¼Œé‡ç‚¹æ¨å¹¿é«˜åˆ©æ¶¦ç‡äº§å“
- åŠ å¼ºå®¢æˆ·å…³ç³»ç®¡ç†ï¼Œæé«˜å®¢æˆ·ç”Ÿå‘½å‘¨æœŸä»·å€¼
- å¹³è¡¡çº¿ä¸Šçº¿ä¸‹æ¸ é“æŠ•å…¥ï¼Œå®ç°å…¨æ¸ é“ååŒå‘å±•"""
        
        return cleaned_text 